import numpy as np
import os
from music21 import converter, instrument, note, chord, stream
from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation
from tensorflow.keras.optimizers import Adam

def get_notes():
    notes = []
    for file in os.listdir("midi_data"):
        midi = converter.parse(os.path.join("midi_data", file))
        parts = instrument.partitionByInstrument(midi)
        if parts:  # File has instrument parts
            notes_to_parse = parts.parts[0].recurse()
        else:  # File has notes in a flat structure
            notes_to_parse = midi.flat.notes

        for element in notes_to_parse:
            if isinstance(element, note.Note):
                notes.append(str(element.pitch))
            elif isinstance(element, chord.Chord):
                notes.append('.'.join(str(n) for n in element.normalOrder))

    return notes

notes = get_notes()

lb = LabelBinarizer()
note_sequences = lb.fit_transform(notes)

sequence_length = 100
n_vocab = len(lb.classes_)

network_input = []
network_output = []

for i in range(0, len(note_sequences) - sequence_length):
    sequence_in = note_sequences[i:i + sequence_length]
    sequence_out = note_sequences[i + sequence_length]
    network_input.append(sequence_in)
    network_output.append(sequence_out)

network_input = np.array(network_input)
network_output = np.array(network_output)

model = Sequential([
    LSTM(256, input_shape=(sequence_length, n_vocab), return_sequences=True),
    Dropout(0.3),
    LSTM(256, return_sequences=True),
    Dropout(0.3),
    LSTM(256),
    Dense(n_vocab),
    Activation('softmax')
])

model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001))

model.fit(network_input, network_output, epochs=50, batch_size=64)


def generate_notes(model, network_input, n_vocab, int_to_note, num_generate=500):
    start = np.random.randint(0, len(network_input)-1)
    pattern = network_input[start]
    prediction_output = []

    for note_index in range(num_generate):
        prediction_input = np.reshape(pattern, (1, len(pattern), n_vocab))
        prediction = model.predict(prediction_input, verbose=0)
        index = np.argmax(prediction)
        result = int_to_note[index]
        prediction_output.append(result)

        pattern = np.append(pattern, index)
        pattern = pattern[1:len(pattern)]

    return prediction_output

int_to_note = {number: note for number, note in enumerate(lb.classes_)}
generated_notes = generate_notes(model, network_input, n_vocab, int_to_note)

def create_midi(prediction_output, filename='output.mid'):
    offset = 0
    output_notes = []

    for pattern in prediction_output:
        if ('.' in pattern) or pattern.isdigit():  # Chord
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        else:  # Note
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        offset += 0.5

    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp=filename)

create_midi(generated_notes)
